{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503efd52-442f-4f28-996d-0dfa0913fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 23:00:14.562580: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-27 23:00:14.567030: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-27 23:00:14.578820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735340414.597506    7970 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735340414.603239    7970 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-27 23:00:14.624089: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "\n",
    "# SARIMA model\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "# LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Seq2seq\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85bbe3ad-fbdd-4873-aa66-0b08a786264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f30ed8c-8bde-4ade-9913-5e8cd18edb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>gdlat</th>\n",
       "      <th>glon</th>\n",
       "      <th>tec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52995</th>\n",
       "      <td>2001-01-01 01:12:30</td>\n",
       "      <td>56.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56835</th>\n",
       "      <td>2001-01-01 01:17:30</td>\n",
       "      <td>56.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56836</th>\n",
       "      <td>2001-01-01 01:17:30</td>\n",
       "      <td>56.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60730</th>\n",
       "      <td>2001-01-01 01:22:30</td>\n",
       "      <td>56.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60761</th>\n",
       "      <td>2001-01-01 01:22:30</td>\n",
       "      <td>57.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>25.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12439042</th>\n",
       "      <td>2016-11-27 06:22:30</td>\n",
       "      <td>58.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12450679</th>\n",
       "      <td>2016-11-27 06:27:30</td>\n",
       "      <td>57.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12450680</th>\n",
       "      <td>2016-11-27 06:27:30</td>\n",
       "      <td>57.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462724</th>\n",
       "      <td>2016-11-27 06:32:30</td>\n",
       "      <td>56.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462853</th>\n",
       "      <td>2016-11-27 06:32:30</td>\n",
       "      <td>57.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>741934 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    datetime  gdlat   glon   tec\n",
       "52995    2001-01-01 01:12:30   56.0  140.0  20.0\n",
       "56835    2001-01-01 01:17:30   56.0  139.0  19.9\n",
       "56836    2001-01-01 01:17:30   56.0  140.0  19.6\n",
       "60730    2001-01-01 01:22:30   56.0  139.0  21.5\n",
       "60761    2001-01-01 01:22:30   57.0  139.0  25.8\n",
       "...                      ...    ...    ...   ...\n",
       "12439042 2016-11-27 06:22:30   58.0  139.0   7.4\n",
       "12450679 2016-11-27 06:27:30   57.0  139.0   7.4\n",
       "12450680 2016-11-27 06:27:30   57.0  140.0   7.4\n",
       "12462724 2016-11-27 06:32:30   56.0  140.0   7.4\n",
       "12462853 2016-11-27 06:32:30   57.0  140.0   7.4\n",
       "\n",
       "[741934 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_siberia = pd.concat([pd.read_parquet(f'data/tec_{i}.parquet.gzip') for i in range(2001, 2017)])\n",
    "df_siberia = df_siberia [(df_siberia['gdlat'] >= 56.0) & (df_siberia['gdlat'] <= 58.0) & (df_siberia['glon'] >= 136.0) & (df_siberia['glon'] <= 140.0)][['datetime', 'gdlat', 'glon', 'tec']]\n",
    "df_siberia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af300add-f655-46b1-ac02-6ef16f6ba9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140396it [00:06, 20653.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Timestamp('2003-05-29 20:00:00'), Timestamp('2003-05-30 18:00:00')),\n",
       " (Timestamp('2003-06-18 04:00:00'), Timestamp('2003-06-19 04:00:00')),\n",
       " (Timestamp('2003-07-11 16:00:00'), Timestamp('2003-07-12 18:00:00')),\n",
       " (Timestamp('2003-08-18 04:00:00'), Timestamp('2003-08-19 12:00:00')),\n",
       " (Timestamp('2003-10-29 06:00:00'), Timestamp('2003-11-01 08:00:00')),\n",
       " (Timestamp('2003-11-20 12:00:00'), Timestamp('2003-11-22 10:00:00')),\n",
       " (Timestamp('2004-01-22 12:00:00'), Timestamp('2004-01-23 12:00:00')),\n",
       " (Timestamp('2004-04-03 18:00:00'), Timestamp('2004-04-04 08:00:00')),\n",
       " (Timestamp('2004-07-25 00:00:00'), Timestamp('2004-07-26 16:00:00')),\n",
       " (Timestamp('2004-07-27 00:00:00'), Timestamp('2004-07-29 18:00:00')),\n",
       " (Timestamp('2004-08-30 14:00:00'), Timestamp('2004-08-31 16:00:00')),\n",
       " (Timestamp('2004-11-07 20:00:00'), Timestamp('2004-11-11 22:00:00')),\n",
       " (Timestamp('2005-01-18 00:00:00'), Timestamp('2005-01-18 18:00:00')),\n",
       " (Timestamp('2005-05-08 12:00:00'), Timestamp('2005-05-09 10:00:00')),\n",
       " (Timestamp('2005-05-15 06:00:00'), Timestamp('2005-05-18 10:00:00')),\n",
       " (Timestamp('2005-05-30 08:00:00'), Timestamp('2005-05-31 08:00:00')),\n",
       " (Timestamp('2005-06-12 18:00:00'), Timestamp('2005-06-13 14:00:00')),\n",
       " (Timestamp('2005-08-24 10:00:00'), Timestamp('2005-08-26 00:00:00')),\n",
       " (Timestamp('2005-08-31 14:00:00'), Timestamp('2005-09-01 14:00:00')),\n",
       " (Timestamp('2005-09-11 04:00:00'), Timestamp('2005-09-14 16:00:00')),\n",
       " (Timestamp('2006-12-14 22:00:00'), Timestamp('2006-12-16 18:00:00')),\n",
       " (Timestamp('2011-08-05 22:00:00'), Timestamp('2011-08-07 00:00:00')),\n",
       " (Timestamp('2011-09-26 16:00:00'), Timestamp('2011-09-27 14:00:00')),\n",
       " (Timestamp('2011-10-24 22:00:00'), Timestamp('2011-10-26 02:00:00')),\n",
       " (Timestamp('2012-03-09 02:00:00'), Timestamp('2012-03-11 06:00:00')),\n",
       " (Timestamp('2012-04-23 20:00:00'), Timestamp('2012-04-25 06:00:00')),\n",
       " (Timestamp('2012-07-15 06:00:00'), Timestamp('2012-07-17 14:00:00')),\n",
       " (Timestamp('2012-10-01 00:00:00'), Timestamp('2012-10-01 20:00:00')),\n",
       " (Timestamp('2012-10-08 06:00:00'), Timestamp('2012-10-09 18:00:00')),\n",
       " (Timestamp('2012-11-14 00:00:00'), Timestamp('2012-11-14 16:00:00')),\n",
       " (Timestamp('2013-03-17 08:00:00'), Timestamp('2013-03-18 10:00:00')),\n",
       " (Timestamp('2013-06-01 02:00:00'), Timestamp('2013-06-01 22:00:00')),\n",
       " (Timestamp('2013-06-28 18:00:00'), Timestamp('2013-06-29 22:00:00')),\n",
       " (Timestamp('2014-02-18 22:00:00'), Timestamp('2014-02-19 18:00:00')),\n",
       " (Timestamp('2015-03-17 12:00:00'), Timestamp('2015-03-20 10:00:00')),\n",
       " (Timestamp('2015-06-22 18:00:00'), Timestamp('2015-06-24 20:00:00')),\n",
       " (Timestamp('2015-10-07 14:00:00'), Timestamp('2015-10-08 10:00:00')),\n",
       " (Timestamp('2015-12-20 12:00:00'), Timestamp('2015-12-21 22:00:00')),\n",
       " (Timestamp('2015-12-31 20:00:00'), Timestamp('2016-01-01 12:00:00')),\n",
       " (Timestamp('2016-10-13 10:00:00'), Timestamp('2016-10-14 10:00:00'))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dst = pd.read_csv('dst/dst2001_2017.csv')[['ut1_unix', 'ut2_unix', 'dst']]\n",
    "df_dst['time'] = 0.5*(df_dst['ut1_unix'] + df_dst['ut2_unix'])\n",
    "df_dst['datetime'] = pd.to_datetime(df_dst['time'], unit='s')\n",
    "df_dst = df_dst.set_index('datetime').drop(['ut1_unix', 'ut2_unix', 'time'], axis=1)\n",
    "df_dst['storm'] = df_dst['dst'] < -50\n",
    "df_dst['major_storm'] = df_dst['dst'] < -100\n",
    "has_major_peak = set()\n",
    "current_storm_no = 1\n",
    "previous_storm = False\n",
    "for ix, row in tqdm(df_dst.iterrows()):\n",
    "    if not row['storm']:\n",
    "        if not previous_storm:\n",
    "            continue\n",
    "        else:\n",
    "            previous_storm = False\n",
    "            current_storm_no += 1     \n",
    "            continue\n",
    "    else:\n",
    "        if previous_storm:\n",
    "            df_dst.loc[ix, 'storm_no'] = current_storm_no\n",
    "            if row['major_storm']:\n",
    "                has_major_peak.add(current_storm_no)\n",
    "                continue           \n",
    "        else:\n",
    "            previous_storm = True\n",
    "            df_dst.loc[ix, 'storm_no'] = current_storm_no\n",
    "            if row['major_storm']:\n",
    "                has_major_peak.add(current_storm_no)\n",
    "                continue\n",
    "storms = []\n",
    "for i in sorted(list(has_major_peak)):\n",
    "    storm = df_dst [df_dst['storm_no'] == i]['dst']\n",
    "    storms.append((storm.index[0].floor('2h'), storm.index[-1].ceil('2h')))\n",
    "storms = [storm for storm in storms if (storm[0] >= datetime(2003, 1, 1))]\n",
    "storms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453058c8-bfe8-4ac5-9a46-3f1aa4dec27b",
   "metadata": {},
   "source": [
    "## Manipulation des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b683363f-6e1a-4748-8431-025e398d66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equilibrate_regions(df):\n",
    "    duplicate = np.max(pd.crosstab(df['gdlat'], df['glon'])) // pd.crosstab(df['gdlat'], df['glon']) - 1\n",
    "    sample = np.max(pd.crosstab(df['gdlat'], df['glon'])) % pd.crosstab(df['gdlat'], df['glon'])\n",
    "    for gdlat in duplicate.index:\n",
    "        for glon in duplicate.columns:\n",
    "            # Duplicate\n",
    "            df_lat_lon = df[ (df['gdlat'] == gdlat) & (df['glon'] == glon) ].reset_index(drop=True)\n",
    "            add = [df_lat_lon]*duplicate.loc[gdlat, glon]\n",
    "            # Sample\n",
    "            sampled_indices = np.random.choice(df_lat_lon.index, size=sample.loc[gdlat, glon], replace=False)\n",
    "            add.append(df_lat_lon.loc[sampled_indices, :])\n",
    "            # Add new rows\n",
    "            if add:\n",
    "                add = pd.concat(add)\n",
    "                df = pd.concat([df, add]).reset_index(drop=True)\n",
    "    return df.sort_values(by='datetime').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96dae7b-8ce3-40de-b354-3c698f43ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_by_2hourly_bin(df, datetime_col, measure_col):\n",
    "    \"\"\"\n",
    "    Average values within 2-hour bins.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing datetime and measure columns\n",
    "    datetime_col : str\n",
    "        Name of the datetime column\n",
    "    measure_col : str\n",
    "        Name of the measure column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with 2-hourly timestamps and averaged values\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create regular 2-hourly time index\n",
    "    start_time = df[datetime_col].min().floor('2h')\n",
    "    end_time = df[datetime_col].max().ceil('2h')\n",
    "    two_hourly_index = pd.date_range(start=start_time, end=end_time, freq='2h')\n",
    "    \n",
    "    # Bin data into 2-hour intervals and calculate mean\n",
    "    df[datetime_col] = pd.to_datetime(df[datetime_col]).dt.floor('2h')\n",
    "    result = df.groupby(datetime_col)[measure_col].mean().reindex(two_hourly_index)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    result_df = result.reset_index()\n",
    "    result_df.columns = [datetime_col, measure_col]\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02a6e211-4adb-469f-a8e0-6f3e336c053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trajectory_matrix(time_series, window):\n",
    "    \"\"\"\n",
    "    Create trajectory matrix from time series using sliding window.\n",
    "    \n",
    "    Args:\n",
    "        time_series (array-like): Input time series\n",
    "        window (int): Window size\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Trajectory matrix\n",
    "    \"\"\"\n",
    "    n = len(time_series)\n",
    "    k = n - window + 1\n",
    "    matrix = np.zeros((k, window))\n",
    "    \n",
    "    for i in range(k):\n",
    "        matrix[i, :] = time_series[i:i + window]\n",
    "        \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "341404a0-4e52-4b66-a6dc-02e3abb39888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_interpolate(X, W, K, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Interpolate missing values in trajectory matrix using low rank approximation.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Trajectory matrix with missing values\n",
    "        W (np.ndarray): Mask matrix (1 for observed, 0 for missing)\n",
    "        K (int): Expected rank\n",
    "        max_iter (int): Maximum iterations\n",
    "        tol (float): Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Interpolated trajectory matrix\n",
    "    \"\"\"\n",
    "    X_hat = np.zeros_like(X)\n",
    "    X_hat[W == 0] = np.mean(X[W == 1])  # Initialize missing values with mean\n",
    "\n",
    "    prev_norm = np.inf\n",
    "    prev_criterion = np.inf\n",
    "    for _ in tqdm(range(max_iter)):\n",
    "        # Combine observed values from X and interpolated values from X_hat\n",
    "        Y = X + X_hat * (1 - W)\n",
    "        \n",
    "        # SVD computation\n",
    "        U, s, Vt = svd(Y, full_matrices=False)\n",
    "        \n",
    "        # Low rank approximation\n",
    "        X_hat = sum(s[k] * np.outer(U[:, k], Vt[k, :]) for k in range(min(K, len(s))))\n",
    "        \n",
    "        # Check convergence\n",
    "        current_norm = np.linalg.norm(X_hat * (1 - W))\n",
    "        current_criterion = abs(current_norm - prev_norm)\n",
    "        if current_criterion > prev_criterion: # diverging\n",
    "            break\n",
    "        if current_criterion < tol:\n",
    "            break\n",
    "        prev_norm = current_norm\n",
    "        prev_criterion = current_criterion\n",
    "        \n",
    "    return X_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d443b21-a2f8-4f8e-acd9-feedaa67c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_time_series(trajectory_matrix):\n",
    "    \"\"\"\n",
    "    Recover original time series from trajectory matrix using diagonal averaging.\n",
    "    \n",
    "    Args:\n",
    "        trajectory_matrix (np.ndarray): Matrix of shape (n - window_size + 1, window_size)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Recovered time series of length n\n",
    "    \"\"\"\n",
    "    L = trajectory_matrix.shape[1]  # window_size\n",
    "    K = trajectory_matrix.shape[0]  # n - window_size + 1\n",
    "    n = L + K - 1  # original series length\n",
    "    \n",
    "    recovered = np.zeros(n)\n",
    "    counts = np.zeros(n)\n",
    "    \n",
    "    # Fill the recovered series using diagonal averaging\n",
    "    for i in range(K):\n",
    "        for j in range(L):\n",
    "            recovered[i + j] += trajectory_matrix[i, j]\n",
    "            counts[i + j] += 1\n",
    "            \n",
    "    # Normalize by the number of elements in each diagonal\n",
    "    recovered = recovered / counts\n",
    "    \n",
    "    return recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38ae32fc-4e80-484c-bb6f-34013cdbda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(train_start, test_start, test_end, k_singular_values=24, trajectory_window=120):\n",
    "    # train\n",
    "    print('Preprocessing train set...')\n",
    "    df_siberia_train = df_siberia [(df_siberia['datetime'] >= train_start) & (df_siberia['datetime'] < test_start)]\n",
    "    df_siberia_train_equilibrate = equilibrate_regions(df_siberia_train)\n",
    "    df_siberia_train_equilibrate_2hourly = average_by_2hourly_bin(df_siberia_train_equilibrate, 'datetime', 'tec').iloc[:-1]\n",
    "    df_siberia_train_equilibrate_2hourly = df_siberia_train_equilibrate_2hourly.set_index('datetime')\n",
    "    X = create_trajectory_matrix(df_siberia_train_equilibrate_2hourly['tec'].to_numpy(), trajectory_window)\n",
    "    W = (~np.isnan(X)).astype(int)\n",
    "    X = np.nan_to_num(X)\n",
    "    estimated_trajectory_matrix = low_rank_interpolate(X, W, k_singular_values) # keep first 24 singular values \n",
    "    train = pd.Series(recover_time_series(estimated_trajectory_matrix), index=df_siberia_train_equilibrate_2hourly.index)\n",
    "\n",
    "    # test\n",
    "    print('Preprocessing test set...')\n",
    "    df_siberia_test = df_siberia [(df_siberia['datetime'] >= train_start) & (df_siberia['datetime'] < test_end)]\n",
    "    df_siberia_test_equilibrate = equilibrate_regions(df_siberia_test)\n",
    "    df_siberia_test_equilibrate_2hourly = average_by_2hourly_bin(df_siberia_test_equilibrate, 'datetime', 'tec').iloc[:-1]\n",
    "    df_siberia_test_equilibrate_2hourly = df_siberia_test_equilibrate_2hourly.set_index('datetime')\n",
    "    X = create_trajectory_matrix(df_siberia_test_equilibrate_2hourly['tec'].to_numpy(), trajectory_window)\n",
    "    W = (~np.isnan(X)).astype(int)\n",
    "    X = np.nan_to_num(X)\n",
    "    estimated_trajectory_matrix = low_rank_interpolate(X, W, k_singular_values) # keep first 24 singular values \n",
    "    test = pd.Series(recover_time_series(estimated_trajectory_matrix), index=df_siberia_test_equilibrate_2hourly.index)\n",
    "    \n",
    "    return train, test.loc[test_start:test_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5370293-04f8-4061-9fc0-b345a9c3ce96",
   "metadata": {},
   "source": [
    "## ModÃ¨les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39391609-de76-443f-b589-3f4d2afda4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/12/2024 23:00:24 Iteration 1/40\n",
      "2003-05-28 20:00:00 2003-06-07 20:00:00\n",
      "Preprocessing train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 7/100 [00:38<08:31,  5.50s/it]"
     ]
    }
   ],
   "source": [
    "results = {'sarima_summary':[], 'sarima_converged':[], 'correlation':[], 'rmse':[]}\n",
    "for ix, (start, end) in enumerate(storms[:3]):\n",
    "    dt_t = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print('{} Iteration {}/{}'.format(dt_t, ix+1, len(storms)))\n",
    "    print(start - timedelta(days=1), start + timedelta(days=9))\n",
    "    \n",
    "    ### Preprocess train and test\n",
    "    train, test = get_train_and_test(train_start=start - timedelta(days=365*2+1), test_start=start - timedelta(days=1), test_end=start + timedelta(days=9))\n",
    "    \n",
    "    ### SARIMA\n",
    "    model = SARIMAX(train, order=(2, 0, 2), seasonal_order=(2, 1, 2, 12)).fit(maxiter=100)\n",
    "    results['sarima_summary'].append(model.summary().tables[1])\n",
    "    print(results['sarima_summary'][-1])\n",
    "    results['sarima_converged'].append(model.mle_retvals['converged'])\n",
    "    print('Sarima estimation converged? :{}'.format(results['sarima_converged'][-1]))\n",
    "    predict = model.forecast(steps=120)\n",
    "    plt.figure(figsize=(20, 1))\n",
    "    plt.plot(test, label='true')\n",
    "    plt.plot(predict, label='predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    results['correlation'].append(pearsonr(test, predict).statistic)\n",
    "    results['rmse'].append(np.sqrt(mean_squared_error(test, predict)))\n",
    "    print('Correlation of the SARIMA model: {}'.format(results['correlation'][-1]))\n",
    "    print('RMSE of the SARIMA model: {}'.format(results['rmse'][-1]))\n",
    "    \n",
    "    ### LSTM\n",
    "\n",
    "    ### Seq2Seq\n",
    "\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ef6ba-1107-4c39-b65c-aa79311e0aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment-cpu",
   "language": "python",
   "name": "environment-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
